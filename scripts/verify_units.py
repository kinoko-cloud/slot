#!/usr/bin/env python3
"""
å°ç•ªå·æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

å¤œé–“ãƒãƒƒãƒï¼ˆdaily_collect.pyï¼‰ã®å¾Œã«å®Ÿè¡Œã—ã€
è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å°ç•ªå·ãƒªã‚¹ãƒˆã¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å®Ÿéš›ã«è¦‹ã¤ã‹ã£ãŸå°ç•ªå·ã‚’ç…§åˆã™ã‚‹ã€‚

æ¤œçŸ¥ã™ã‚‹ã‚±ãƒ¼ã‚¹:
- å°æ’¤å»: configå®šç¾©ã«ã‚ã‚‹å°ç•ªå·ãŒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã«å­˜åœ¨ã—ãªã„
- æ–°å°è¿½åŠ : ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµæœã«configæœªå®šç¾©ã®å°ç•ªå·ãŒã‚ã‚‹
- å°æ•°å¤‰å‹•: è¨­ç½®å°æ•°ãŒå¢—æ¸›ã—ãŸ
- æ©Ÿç¨®åä¸ä¸€è‡´: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å–ã‚ŒãŸæ©Ÿç¨®åã¨æœŸå¾…å€¤ãŒç•°ãªã‚‹

å‡ºåŠ›: data/alerts/ ã«JSONå½¢å¼ã§ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ä¿å­˜
"""

import json
import sys
from datetime import datetime, timezone, timedelta
from pathlib import Path

PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from config.rankings import STORES, MACHINES

JST = timezone(timedelta(hours=9))
ALERTS_DIR = PROJECT_ROOT / 'data' / 'alerts'


def verify_units_from_daily(daily_data: dict) -> list:
    """ãƒ‡ã‚¤ãƒªãƒ¼åé›†çµæœã‹ã‚‰å°ç•ªå·ã‚’æ¤œè¨¼ã™ã‚‹

    Args:
        daily_data: daily_collect.pyã®å‡ºåŠ›çµæœï¼ˆstoresè¾æ›¸ã‚’å«ã‚€ï¼‰

    Returns:
        ã‚¢ãƒ©ãƒ¼ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    alerts = []
    checked_at = datetime.now(JST).isoformat()

    stores_data = daily_data.get('stores', {})

    for result_key, store_data in stores_data.items():
        # result_keyã¯ "shibuya_espass_sbj" ã®ã‚ˆã†ãªå½¢å¼
        # config/rankings.pyã®STORESã‚­ãƒ¼ã¨ç…§åˆ
        store_config = STORES.get(result_key)
        if not store_config:
            # æ—§å½¢å¼ã‚­ãƒ¼ã®å ´åˆã¯å¯¾å¿œã™ã‚‹STORESã‚­ãƒ¼ã‚’æ¢ã™
            store_config = _find_store_config(result_key, store_data.get('machine_key'))
            if not store_config:
                continue

        config_units = set(store_config.get('units', []))
        machine_key = store_config.get('machine', '')
        machine_name = MACHINES.get(machine_key, {}).get('name', '')
        store_name = store_config.get('name', result_key)

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§å®Ÿéš›ã«å–å¾—ã—ãŸå°ç•ªå·
        scraped_units = set()
        scraped_machine_names = set()
        for unit in store_data.get('units', []):
            unit_id = str(unit.get('unit_id', ''))
            if unit_id:
                scraped_units.add(unit_id)
            m_name = unit.get('machine_name', '')
            if m_name:
                scraped_machine_names.add(m_name)

        if not scraped_units:
            # ãƒ‡ãƒ¼ã‚¿ãŒå–ã‚Œãªã‹ã£ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ã®å¯èƒ½æ€§ï¼‰
            continue

        # 1. æ¶ˆãˆãŸå°ç•ªå·ï¼ˆå°æ’¤å» or å°ç§»å‹•ï¼‰
        missing_units = config_units - scraped_units
        if missing_units:
            alerts.append({
                'type': 'unit_missing',
                'severity': 'warning',
                'store_key': result_key,
                'store_name': store_name,
                'machine_key': machine_key,
                'units': sorted(missing_units),
                'message': f'{store_name}: å°ç•ªå· {", ".join(sorted(missing_units))} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆå°æ’¤å»ã¾ãŸã¯å°ç§»å‹•ã®å¯èƒ½æ€§ï¼‰',
                'checked_at': checked_at,
            })

        # 2. æ–°ã—ã„å°ç•ªå·ï¼ˆå°è¿½åŠ  or å°ç§»å‹•å…ˆï¼‰
        new_units = scraped_units - config_units
        if new_units:
            alerts.append({
                'type': 'unit_new',
                'severity': 'info',
                'store_key': result_key,
                'store_name': store_name,
                'machine_key': machine_key,
                'units': sorted(new_units),
                'message': f'{store_name}: æ–°ã—ã„å°ç•ªå· {", ".join(sorted(new_units))} ã‚’æ¤œå‡ºï¼ˆæ–°å°è¿½åŠ ã¾ãŸã¯å°ç§»å‹•ã®å¯èƒ½æ€§ï¼‰',
                'checked_at': checked_at,
            })

        # 3. å°æ•°å¤‰å‹•
        if len(scraped_units) != len(config_units):
            diff = len(scraped_units) - len(config_units)
            direction = 'å¢—åŠ ' if diff > 0 else 'æ¸›å°‘'
            alerts.append({
                'type': 'unit_count_change',
                'severity': 'warning' if diff < 0 else 'info',
                'store_key': result_key,
                'store_name': store_name,
                'machine_key': machine_key,
                'config_count': len(config_units),
                'actual_count': len(scraped_units),
                'diff': diff,
                'message': f'{store_name}: å°æ•°{direction}ï¼ˆè¨­å®š{len(config_units)}å° â†’ å®Ÿéš›{len(scraped_units)}å°ï¼‰',
                'checked_at': checked_at,
            })

        # 4. åŒæ™‚ã«æ¶ˆãˆãŸå°ã¨æ–°ã—ã„å°ãŒã‚ã‚‹å ´åˆâ†’å°ç§»å‹•ã®å¯èƒ½æ€§
        if missing_units and new_units:
            alerts.append({
                'type': 'unit_move_suspected',
                'severity': 'warning',
                'store_key': result_key,
                'store_name': store_name,
                'machine_key': machine_key,
                'missing': sorted(missing_units),
                'new': sorted(new_units),
                'message': f'{store_name}: å°ç§»å‹•ã®å¯èƒ½æ€§ â€” æ¶ˆå¤±: {", ".join(sorted(missing_units))} / æ–°è¦: {", ".join(sorted(new_units))}',
                'checked_at': checked_at,
            })

    return alerts


def verify_units_from_availability(availability_data: dict) -> list:
    """availability.jsonï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ï¼‰ã‹ã‚‰å°ç•ªå·ã‚’æ¤œè¨¼ã™ã‚‹

    ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å–å¾—æ™‚ã«ã‚‚ç°¡æ˜“ãƒã‚§ãƒƒã‚¯ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

    Args:
        availability_data: availability.jsonã®å†…å®¹

    Returns:
        ã‚¢ãƒ©ãƒ¼ãƒˆã®ãƒªã‚¹ãƒˆ
    """
    alerts = []
    checked_at = datetime.now(JST).isoformat()

    stores_data = availability_data.get('stores', {})

    for store_key, store_data in stores_data.items():
        store_config = STORES.get(store_key)
        if not store_config:
            continue

        config_units = set(store_config.get('units', []))
        store_name = store_config.get('name', store_key)

        # availability.jsonã‹ã‚‰å°ç•ªå·ã‚’å–å¾—
        scraped_units = set()
        for unit in store_data.get('units', []):
            unit_id = str(unit.get('unit_id', ''))
            if unit_id:
                scraped_units.add(unit_id)

        # empty/playing ãƒªã‚¹ãƒˆã‹ã‚‰ã‚‚å–å¾—
        for u in store_data.get('empty', []):
            scraped_units.add(str(u))
        for u in store_data.get('playing', []):
            scraped_units.add(str(u))

        if not scraped_units:
            continue

        missing = config_units - scraped_units
        new = scraped_units - config_units

        if missing:
            alerts.append({
                'type': 'unit_missing',
                'severity': 'warning',
                'store_key': store_key,
                'store_name': store_name,
                'units': sorted(missing),
                'message': f'{store_name}: å°ç•ªå· {", ".join(sorted(missing))} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“',
                'checked_at': checked_at,
            })

        if new:
            alerts.append({
                'type': 'unit_new',
                'severity': 'info',
                'store_key': store_key,
                'store_name': store_name,
                'units': sorted(new),
                'message': f'{store_name}: æ–°ã—ã„å°ç•ªå· {", ".join(sorted(new))} ã‚’æ¤œå‡º',
                'checked_at': checked_at,
            })

    return alerts


def save_alerts(alerts: list, source: str = 'daily') -> Path:
    """ã‚¢ãƒ©ãƒ¼ãƒˆã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹

    Args:
        alerts: ã‚¢ãƒ©ãƒ¼ãƒˆã®ãƒªã‚¹ãƒˆ
        source: ã‚¢ãƒ©ãƒ¼ãƒˆå…ƒï¼ˆ'daily' or 'availability'ï¼‰

    Returns:
        ä¿å­˜å…ˆãƒ‘ã‚¹
    """
    ALERTS_DIR.mkdir(parents=True, exist_ok=True)

    now = datetime.now(JST)
    filename = f'alerts_{source}_{now.strftime("%Y%m%d_%H%M")}.json'
    output_path = ALERTS_DIR / filename

    alert_data = {
        'generated_at': now.isoformat(),
        'source': source,
        'alert_count': len(alerts),
        'alerts': alerts,
    }

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(alert_data, f, ensure_ascii=False, indent=2)

    return output_path


def load_latest_alerts() -> dict:
    """æœ€æ–°ã®ã‚¢ãƒ©ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€

    Returns:
        ã‚¢ãƒ©ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆãªã‘ã‚Œã°ç©ºè¾æ›¸ï¼‰
    """
    if not ALERTS_DIR.exists():
        return {}

    alert_files = sorted(ALERTS_DIR.glob('alerts_*.json'), reverse=True)
    if not alert_files:
        return {}

    with open(alert_files[0], 'r', encoding='utf-8') as f:
        return json.load(f)


def get_active_alerts() -> list:
    """ç¾åœ¨æœ‰åŠ¹ãªã‚¢ãƒ©ãƒ¼ãƒˆã‚’å–å¾—ã™ã‚‹

    24æ™‚é–“ä»¥å†…ã®warningä»¥ä¸Šã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’è¿”ã™ã€‚

    Returns:
        æœ‰åŠ¹ãªã‚¢ãƒ©ãƒ¼ãƒˆãƒªã‚¹ãƒˆ
    """
    latest = load_latest_alerts()
    if not latest:
        return []

    # 24æ™‚é–“ä»¥å†…ã®ã‚¢ãƒ©ãƒ¼ãƒˆã®ã¿
    generated_at = latest.get('generated_at', '')
    if generated_at:
        try:
            gen_time = datetime.fromisoformat(generated_at)
            now = datetime.now(JST)
            if (now - gen_time).total_seconds() > 86400:
                return []
        except:
            pass

    return [a for a in latest.get('alerts', []) if a.get('severity') in ('warning', 'critical')]


def get_unit_status(store_key: str, unit_id: str) -> dict:
    """ç‰¹å®šã®å°ã®ã‚¢ãƒ©ãƒ¼ãƒˆçŠ¶æ…‹ã‚’å–å¾—ã™ã‚‹

    Returns:
        {'status': 'normal'/'missing'/'new'/'moved', 'message': str}
    """
    active = get_active_alerts()

    for alert in active:
        if alert.get('store_key') != store_key:
            continue

        if alert.get('type') == 'unit_move_suspected':
            if unit_id in alert.get('missing', []):
                return {'status': 'moved', 'message': 'å°ç§»å‹•ï¼ˆæ¶ˆå¤±ï¼‰'}
            if unit_id in alert.get('new', []):
                return {'status': 'moved', 'message': 'å°ç§»å‹•ï¼ˆæ–°è¨­ï¼‰'}

        if alert.get('type') == 'unit_missing' and unit_id in alert.get('units', []):
            return {'status': 'missing', 'message': 'å°æ’¤å»ã®å¯èƒ½æ€§'}

        if alert.get('type') == 'unit_new' and unit_id in alert.get('units', []):
            return {'status': 'new', 'message': 'æ–°å°'}

    return {'status': 'normal', 'message': ''}


def _find_store_config(result_key: str, machine_key: str = None):
    """daily_collectã®çµæœã‚­ãƒ¼ã‹ã‚‰STORESè¨­å®šã‚’æ¢ã™"""
    # ã¾ãšç›´æ¥ãƒãƒƒãƒ
    if result_key in STORES:
        return STORES[result_key]

    # result_keyãŒ "shibuya_espass_sbj" ã®å ´åˆã¯ãã®ã¾ã¾
    # "shibuya_espass" + machine_key = "sbj" â†’ "shibuya_espass_sbj"
    if machine_key:
        combined = f"{result_key}_{machine_key}"
        if combined in STORES:
            return STORES[combined]

    return None


def print_report(alerts: list):
    """ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›"""
    if not alerts:
        print('âœ“ å°ç•ªå·ã®ç•°å¸¸ãªã—')
        return

    print(f'\n{"="*50}')
    print(f'å°ç•ªå·æ¤œè¨¼çµæœ: {len(alerts)}ä»¶ã®ã‚¢ãƒ©ãƒ¼ãƒˆ')
    print(f'{"="*50}')

    for alert in alerts:
        severity = alert.get('severity', 'info')
        icon = 'ğŸ”´' if severity == 'critical' else 'ğŸŸ¡' if severity == 'warning' else 'ğŸ”µ'
        print(f'\n{icon} [{alert["type"]}] {alert["message"]}')

    print()


def main():
    """ã‚¹ã‚¿ãƒ³ãƒ‰ã‚¢ãƒ­ãƒ³å®Ÿè¡Œ: availability.jsonã‹ã‚‰æ¤œè¨¼"""
    import argparse
    parser = argparse.ArgumentParser(description='å°ç•ªå·æ¤œè¨¼')
    parser.add_argument('--source', choices=['availability', 'daily'], default='availability',
                        help='æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹')
    parser.add_argument('--daily-file', type=str, help='ãƒ‡ã‚¤ãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆ--source dailyæ™‚ï¼‰')
    args = parser.parse_args()

    if args.source == 'availability':
        avail_path = PROJECT_ROOT / 'data' / 'availability.json'
        if not avail_path.exists():
            print('availability.json ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
            return

        with open(avail_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        alerts = verify_units_from_availability(data)

    elif args.source == 'daily':
        if args.daily_file:
            daily_path = Path(args.daily_file)
        else:
            # æœ€æ–°ã®ãƒ‡ã‚¤ãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
            daily_dir = PROJECT_ROOT / 'data' / 'daily'
            daily_files = sorted(daily_dir.glob('daily_*.json'), reverse=True)
            if not daily_files:
                print('ãƒ‡ã‚¤ãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
                return
            daily_path = daily_files[0]

        with open(daily_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        alerts = verify_units_from_daily(data)

    print_report(alerts)

    if alerts:
        save_path = save_alerts(alerts, source=args.source)
        print(f'ã‚¢ãƒ©ãƒ¼ãƒˆä¿å­˜: {save_path}')


if __name__ == '__main__':
    main()
