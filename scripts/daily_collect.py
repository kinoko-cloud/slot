#!/usr/bin/env python3
"""
ãƒ‡ã‚¤ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

æ¯æ—¥å®Ÿè¡Œã—ã¦å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã‚’è“„ç©
- å‰æ—¥ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
- JSONã§æ—¥ä»˜åˆ¥ã«ä¿å­˜
- é•·æœŸçš„ãªå‚¾å‘åˆ†æç”¨
- æ’ä»–ãƒ­ãƒƒã‚¯ä»˜ã â€” è¤‡æ•°ãƒ—ãƒ­ã‚»ã‚¹ã®åŒæ™‚å®Ÿè¡Œã‚’é˜²æ­¢
"""
# æ’ä»–ãƒ­ãƒƒã‚¯(æœ€åˆã«å–å¾—)
import sys, os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)) + '/..')
from scripts.fetch_lock import acquire_lock
_lock_fp = acquire_lock()

å¯¾å¿œæ©Ÿç¨®:
- SBJ (ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ–ãƒ©ãƒƒã‚¯ã‚¸ãƒ£ãƒƒã‚¯)
- åŒ—æ–—è»¢ç”Ÿ2 (åŒ—æ–—ã®æ‹³ è»¢ç”Ÿã®ç« 2)
"""

import json
from datetime import datetime, timedelta
from pathlib import Path
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

from config.stores import DAIDATA_STORES, PAPIMO_STORES, MACHINES
from scrapers.daidata_detail_history import get_all_history
from scripts.verify_units import verify_units_from_daily, save_alerts, print_report


def collect_daily_data(machine_keys: list = None, max_units_per_store: int = None):
    """å…¨åº—èˆ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†

    Args:
        machine_keys: å–å¾—ã™ã‚‹æ©Ÿç¨®ãƒªã‚¹ãƒˆ(None=å…¨æ©Ÿç¨®)
        max_units_per_store: åº—èˆ—ã‚ãŸã‚Šã®æœ€å¤§å°æ•°(ãƒ†ã‚¹ãƒˆç”¨)
    """
    today = datetime.now().strftime('%Y%m%d')
    data_dir = Path('data/daily')
    data_dir.mkdir(parents=True, exist_ok=True)

    if machine_keys is None:
        machine_keys = ['sbj', 'hokuto_tensei2']

    results = {
        'collected_at': datetime.now().isoformat(),
        'machines': machine_keys,
        'stores': {}
    }

    print('=' * 60)
    print(f'ãƒ‡ã‚¤ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿åé›† - {datetime.now().strftime("%Y-%m-%d %H:%M")}')
    print(f'å¯¾è±¡æ©Ÿç¨®: {", ".join(machine_keys)}')
    print('=' * 60)

    # 1. å°ãƒ‡ãƒ¼ã‚¿ã‚ªãƒ³ãƒ©ã‚¤ãƒ³åº—èˆ—(å½“æ—¥åˆ†ã‚‚å«ã‚å…¨æ—¥ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—)
    for store_key, store_config in DAIDATA_STORES.items():
        hall_id = store_config['hall_id']
        hall_name = store_config['name']

        for machine_key in machine_keys:
            units = store_config.get('machines', {}).get(machine_key, [])
            if not units:
                continue

            if max_units_per_store:
                units = units[:max_units_per_store]

            machine_name = MACHINES.get(machine_key, {}).get('name', machine_key)
            print(f'\nã€{hall_name} - {machine_name}ã€‘')

            try:
                collected = []
                _expected = MACHINES.get(machine_key, {}).get('verify_keywords')
                for unit_id in units:
                    print(f'  å°{unit_id}...')
                    result = get_all_history(hall_id=hall_id, unit_id=unit_id, hall_name=hall_name,
                                             expected_machine=_expected)
                    if result and result.get('machine_mismatch'):
                        print(f"    âš ï¸ å°{unit_id}ã¯åˆ¥æ©Ÿç¨®ã«å¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ã€‚åé›†ã‚¹ã‚­ãƒƒãƒ—ã€‚")
                        continue
                    if result:
                        # ç©ºãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯: daysãŒç©º or å…¨æ—¥art=0&games=0ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—
                        valid_days = [d for d in result.get('days', [])
                                      if d.get('art', 0) > 0 or (d.get('total_start', 0) or d.get('games', 0) or 0) > 0]
                        if not valid_days:
                            print(f"    âš ï¸ å°{unit_id}: æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãªã—(ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—ã®å¯èƒ½æ€§)")
                            continue
                        result['machine_key'] = machine_key
                        result['machine_name'] = machine_name
                        collected.append(result)

                result_key = f'{store_key}_{machine_key}'
                results['stores'][result_key] = {
                    'hall_name': hall_name,
                    'machine_key': machine_key,
                    'machine_name': machine_name,
                    'units': collected,
                }
                print(f'  âœ“ {len(collected)}å°å–å¾—')

            except Exception as e:
                print(f'  âœ— ã‚¨ãƒ©ãƒ¼: {e}')

    # 2. PAPIMOåº—èˆ—
    from playwright.sync_api import sync_playwright
    from scrapers.papimo import get_unit_history

    for store_key, store_config in PAPIMO_STORES.items():
        hall_id = store_config['hall_id']
        hall_name = store_config['name']

        for machine_key in machine_keys:
            units = store_config.get('machines', {}).get(machine_key, [])
            if not units:
                continue

            if max_units_per_store:
                units = units[:max_units_per_store]

            machine_name = MACHINES.get(machine_key, {}).get('name', machine_key)
            print(f'\nã€{hall_name} - {machine_name}ã€‘')

            try:
                collected = []
                with sync_playwright() as p:
                    browser = p.chromium.launch(headless=True)
                    page = browser.new_page()

                    for unit_id in units:
                        print(f'  å°{unit_id}...')
                        # å½“æ—¥åˆ†(23:00å®Ÿè¡Œæ™‚ç‚¹ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿)+ å‰æ—¥åˆ†ã‚’å–å¾—
                        # å·®åˆ†æ›´æ–°: historyã«æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°2æ—¥åˆ†(å½“æ—¥+å‰æ—¥)
                        # æ–°è¦å°ã¯ãƒ•ãƒ«ãƒãƒƒã‚¯ãƒ•ã‚£ãƒ«(14æ—¥åˆ†)
                        from pathlib import Path as _P
                        _hist_dir = _P(f'data/history/{store_key}_{machine_key}')
                        _hist_file = _hist_dir / f'{unit_id}.json'
                        _days_back = 2 if _hist_file.exists() else 14
                        _expected = MACHINES.get(machine_key, {}).get('verify_keywords')
                        result = get_unit_history(page, hall_id, unit_id, days_back=_days_back,
                                                  expected_machine=_expected)
                        if result.get('machine_mismatch'):
                            print(f"    âš ï¸ å°{unit_id}ã¯åˆ¥æ©Ÿç¨®ã«å¤‰æ›´ã•ã‚ŒãŸå¯èƒ½æ€§ã€‚åé›†ã‚¹ã‚­ãƒƒãƒ—ã€‚")
                            continue
                        # ç©ºãƒ‡ãƒ¼ã‚¿ãƒã‚§ãƒƒã‚¯
                        valid_days = [d for d in result.get('days', [])
                                      if d.get('art', 0) > 0 or (d.get('total_start', 0) or d.get('games', 0) or 0) > 0]
                        if not valid_days:
                            print(f"    âš ï¸ å°{unit_id}: æœ‰åŠ¹ãƒ‡ãƒ¼ã‚¿ãªã—(ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—ã®å¯èƒ½æ€§)")
                            continue
                        result['hall_name'] = hall_name
                        result['machine_key'] = machine_key
                        result['machine_name'] = machine_name
                        collected.append(result)

                    browser.close()

                result_key = f'{store_key}_{machine_key}'
                results['stores'][result_key] = {
                    'hall_name': hall_name,
                    'machine_key': machine_key,
                    'machine_name': machine_name,
                    'units': collected,
                }
                print(f'  âœ“ {len(collected)}å°å–å¾—')

            except Exception as e:
                print(f'  âœ— ã‚¨ãƒ©ãƒ¼: {e}')

    # ä¿å­˜
    machine_suffix = '_'.join(machine_keys) if len(machine_keys) <= 2 else 'all'
    save_path = data_dir / f'daily_{machine_suffix}_{today}.json'
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f'\nâœ“ ä¿å­˜: {save_path}')

    return results


def merge_historical_data():
    """éå»ã®ãƒ‡ã‚¤ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦é•·æœŸåˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ"""
    data_dir = Path('data/daily')
    merged_dir = Path('data/merged')
    merged_dir.mkdir(parents=True, exist_ok=True)

    # å…¨ãƒ‡ã‚¤ãƒªãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    daily_files = sorted(data_dir.glob('sbj_daily_*.json'))

    if not daily_files:
        print('ãƒ‡ã‚¤ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“')
        return

    print(f'çµ±åˆå¯¾è±¡: {len(daily_files)}æ—¥åˆ†')

    # å°ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
    merged = {}

    for file_path in daily_files:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        for store_key, store_data in data.get('stores', {}).items():
            hall_name = store_data.get('hall_name', '')

            for unit in store_data.get('units', []):
                unit_id = unit.get('unit_id')
                key = f"{store_key}_{unit_id}"

                if key not in merged:
                    merged[key] = {
                        'unit_id': unit_id,
                        'hall_name': hall_name,
                        'machine_name': unit.get('machine_name', 'SBJ'),
                        'days': [],
                    }

                # æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                for day in unit.get('days', []):
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    existing_dates = [d.get('date') for d in merged[key]['days']]
                    if day.get('date') not in existing_dates:
                        merged[key]['days'].append(day)

    # æ—¥ä»˜é †ã«ã‚½ãƒ¼ãƒˆ
    for key in merged:
        merged[key]['days'].sort(key=lambda d: d.get('date', ''), reverse=True)

    # ä¿å­˜
    save_path = merged_dir / f'sbj_merged_{datetime.now().strftime("%Y%m%d")}.json'
    with open(save_path, 'w', encoding='utf-8') as f:
        json.dump(list(merged.values()), f, ensure_ascii=False, indent=2)

    print(f'âœ“ çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜: {save_path}')
    print(f'  å°æ•°: {len(merged)}')

    # çµ±è¨ˆè¡¨ç¤º
    for key, data in merged.items():
        days = len(data['days'])
        print(f'  {data["hall_name"]} å°{data["unit_id"]}: {days}æ—¥åˆ†')


def main():
    import argparse
    parser = argparse.ArgumentParser(description='ãƒ‘ãƒã‚¹ãƒ­ ãƒ‡ã‚¤ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿åé›†')
    parser.add_argument('--merge', action='store_true', help='éå»ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ')
    parser.add_argument('--machine', '-m', nargs='+', choices=['sbj', 'hokuto_tensei2', 'all'],
                        default=['all'], help='å–å¾—ã™ã‚‹æ©Ÿç¨® (default: all)')
    parser.add_argument('--max-units', type=int, default=None,
                        help='åº—èˆ—ã‚ãŸã‚Šã®æœ€å¤§å°æ•°(ãƒ†ã‚¹ãƒˆç”¨)')
    args = parser.parse_args()

    if args.merge:
        merge_historical_data()
    else:
        # æ©Ÿç¨®æŒ‡å®š
        if 'all' in args.machine:
            machine_keys = ['sbj', 'hokuto_tensei2']
        else:
            machine_keys = args.machine

        results = collect_daily_data(machine_keys=machine_keys, max_units_per_store=args.max_units)

        # å°ç•ªå·æ¤œè¨¼(å¾“æ¥)
        print('\n' + '=' * 60)
        print('å°ç•ªå·æ¤œè¨¼')
        print('=' * 60)
        alerts = verify_units_from_daily(results)
        print_report(alerts)
        if alerts:
            save_path = save_alerts(alerts, source='daily')
            print(f'ã‚¢ãƒ©ãƒ¼ãƒˆä¿å­˜: {save_path}')

        # åŒ…æ‹¬ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯(æ–°)
        try:
            from scripts.data_integrity_check import run_all_checks, save_check_result, format_notification
            integrity_alerts = run_all_checks(results)
            if integrity_alerts:
                save_check_result(integrity_alerts)
            # é‡å¤§ã‚¢ãƒ©ãƒ¼ãƒˆãŒã‚ã‚Œã°é€šçŸ¥ãƒ†ã‚­ã‚¹ãƒˆå‡ºåŠ›(GitHub Actionsç­‰ã§æ‹¾ãˆã‚‹)
            notification = format_notification(integrity_alerts)
            if notification:
                # é€šçŸ¥ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜(å¤–éƒ¨ã‹ã‚‰èª­ã‚ã‚‹å½¢ã§)
                notif_path = Path('data/alerts/latest_notification.txt')
                notif_path.parent.mkdir(parents=True, exist_ok=True)
                notif_path.write_text(notification, encoding='utf-8')
                print(f'\nğŸ“± é‡å¤§ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥ãƒ†ã‚­ã‚¹ãƒˆä¿å­˜: {notif_path}')
        except Exception as e:
            print(f'âš  æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}')

        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å–å¾—(å·®ç‰TOP10)
        print('\n' + '=' * 60)
        print('ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿å–å¾—(å·®ç‰TOP10)')
        print('=' * 60)
        try:
            from scrapers.daidata_ranking import collect_all_rankings
            ranking_results = collect_all_rankings()
            print(f'ãƒ©ãƒ³ã‚­ãƒ³ã‚°å–å¾—å®Œäº†: {len(ranking_results)}åº—èˆ—')
        except Exception as e:
            print(f'âš  ãƒ©ãƒ³ã‚­ãƒ³ã‚°å–å¾—ã‚¨ãƒ©ãƒ¼: {e}')

        # ã‚µã‚¤ãƒˆã‚»ãƒ–ãƒ³(å…¨å°ã®æœ€é«˜å‡ºç‰ãƒ‡ãƒ¼ã‚¿)
        print('\n' + '=' * 60)
        print('ã‚µã‚¤ãƒˆã‚»ãƒ–ãƒ³ ãƒ‡ãƒ¼ã‚¿å–å¾—(å…¨å°BB/RB/ART/æœ€é«˜å‡ºç‰)')
        print('=' * 60)
        try:
            from scrapers.site777 import collect_and_save as s777_collect
            s777_results = s777_collect(days_back=1)
            total = sum(len(units) for store in s777_results.values() for units in store.values())
            print(f'ã‚µã‚¤ãƒˆã‚»ãƒ–ãƒ³å®Œäº†: {len(s777_results)}åº—èˆ— è¨ˆ{total}å°åˆ†')
        except Exception as e:
            print(f'âš  ã‚µã‚¤ãƒˆã‚»ãƒ–ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {e}')

        # ã‚¢ãƒŠã‚¹ãƒ­(åº—èˆ—æ—¥åˆ¥ ç·å·®æšãƒ»å‹ç‡ãƒ»æ—§ã‚¤ãƒ™ãƒ³ãƒˆæ—¥)
        print('\n' + '=' * 60)
        print('ã‚¢ãƒŠã‚¹ãƒ­ ãƒ‡ãƒ¼ã‚¿å–å¾—(åº—èˆ—æ—¥åˆ¥ å·®æšãƒ»å‹ç‡)')
        print('=' * 60)
        try:
            from scrapers.anaslo import collect_and_save as anaslo_collect
            anaslo_results = anaslo_collect()
            print(f'ã‚¢ãƒŠã‚¹ãƒ­å®Œäº†: {len(anaslo_results)}åº—èˆ—')
        except Exception as e:
            print(f'âš  ã‚¢ãƒŠã‚¹ãƒ­å–å¾—ã‚¨ãƒ©ãƒ¼: {e}')


if __name__ == '__main__':
    main()
